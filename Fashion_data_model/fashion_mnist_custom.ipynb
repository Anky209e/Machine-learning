{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 132,
   "source": [
    "import torch\r\n",
    "from torchvision.transforms import ToTensor\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import torch.nn as nn\r\n",
    "import torch.nn.functional as F\r\n",
    "from torch.utils.data.dataloader import DataLoader\r\n",
    "from torchvision.datasets import FashionMNIST"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "source": [
    "train_dataset = FashionMNIST(root= 'data/', download=True, train=True,transform=ToTensor())\r\n",
    "test_dataset = FashionMNIST(root='data/', train=False,transform=ToTensor())\r\n",
    "len(train_dataset),len(test_dataset)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(60000, 10000)"
      ]
     },
     "metadata": {},
     "execution_count": 133
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "source": [
    "batch_size = 60"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "source": [
    "# dataloader\r\n",
    "train_loader = DataLoader(train_dataset,batch_size, shuffle=True,num_workers=2,pin_memory=True)\r\n",
    "test_loader = DataLoader(test_dataset,batch_size, pin_memory=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "source": [
    "class FashionNeural(nn.Module):\r\n",
    "    def __init__(self,in_size,hidden_size,num_classes):\r\n",
    "        super().__init__()\r\n",
    "        self.linear1 = nn.Linear(in_size,hidden_size)\r\n",
    "        self.relu = nn.ReLU()\r\n",
    "        self.linear2 = nn.Linear(hidden_size,num_classes)\r\n",
    "\r\n",
    "    def forward(self,x):\r\n",
    "        out = self.linear1(x)\r\n",
    "        out = self.relu(out)\r\n",
    "        out = self.linear2(out)\r\n",
    "        return out"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "source": [
    "input_size = 28*28\r\n",
    "hidden_size = 32\r\n",
    "num_classes = 10"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "source": [
    "model = FashionNeural(input_size,hidden_size=hidden_size,num_classes=num_classes)\r\n",
    "model"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "FashionNeural(\n",
       "  (linear1): Linear(in_features=784, out_features=32, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (linear2): Linear(in_features=32, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 138
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.01)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available else 'cpu')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "source": [
    "num_epochs = 5\r\n",
    "\r\n",
    "for epochs in range(num_epochs):\r\n",
    "\r\n",
    "    for images, labels in train_loader:\r\n",
    "        \r\n",
    "        images = images.reshape(-1,28*28)\r\n",
    "        out = model(images)\r\n",
    "        loss = F.cross_entropy(out,labels)\r\n",
    "\r\n",
    "        optimizer.zero_grad()\r\n",
    "        loss.backward()\r\n",
    "        optimizer.step()\r\n",
    "    if(epochs+1)%1== 0:\r\n",
    "        print(f'Epochs:[{epochs}]. Loss:{loss.item()}')\r\n",
    "        "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epochs:[0]. Loss:0.662214457988739\n",
      "Epochs:[1]. Loss:0.3435382544994354\n",
      "Epochs:[2]. Loss:0.3292246460914612\n",
      "Epochs:[3]. Loss:0.4294424057006836\n",
      "Epochs:[4]. Loss:0.4036397635936737\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "source": [
    "with torch.no_grad():\r\n",
    "    n_samples = 0\r\n",
    "    n_correct = 0\r\n",
    "    for images,labels in test_loader:\r\n",
    "        images = images.reshape(-1,28*28)\r\n",
    "        outputs = model(images)\r\n",
    "\r\n",
    "        _, preds = torch.max(outputs, 1)\r\n",
    "\r\n",
    "        n_samples += labels.shape[0]\r\n",
    "        n_correct += (preds == labels).sum().item()\r\n",
    "    \r\n",
    "acc = (n_correct/n_samples)*100\r\n",
    "print(f'Accuracy: {acc}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy: 84.31\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "source": [
    "def predict(img,model):\r\n",
    "    xb = img.unsqueeze(0)\r\n",
    "    xb = xb.reshape(-1,784)\r\n",
    "    yb = model(xb)\r\n",
    "    _,predicts = torch.max(yb, dim=1)\r\n",
    "    return predicts[0].item()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "source": [
    "img, label = test_dataset[323]\r\n",
    "plt.imshow(img[0], cmap='gray')\r\n",
    "print('Label:', train_dataset.classes[label], ', Predicted:', train_dataset.classes[predict(img, model)])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Label: T-shirt/top , Predicted: Dress\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit"
  },
  "interpreter": {
   "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}