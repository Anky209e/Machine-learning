{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "DATA_DIR = 'painter_data'\n",
    "print(os.listdir(DATA_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 64\n",
    "batch_size = 128\n",
    "stats = (0.5, 0.5, 0.5), (0.5, 0.5, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ImageFolder(DATA_DIR, transform=T.Compose([\n",
    "    T.Resize(image_size),\n",
    "    T.CenterCrop(image_size),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(*stats)]))\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=3, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denorm(img_tensors):\n",
    "    return img_tensors * stats[1][0] + stats[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(images, nmax=64):\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    ax.set_xticks([]); ax.set_yticks([])\n",
    "    ax.imshow(make_grid(denorm(images.detach()[:nmax]), nrow=8).permute(1, 2, 0))\n",
    "\n",
    "def show_batch(dl, nmax=64):\n",
    "    for images, _ in dl:\n",
    "        show_images(images, nmax)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_batch(train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_device():\n",
    "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "    \n",
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "class DeviceDataLoader():\n",
    "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "        \n",
    "    def __iter__(self):\n",
    "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
    "        for b in self.dl: \n",
    "            yield to_device(b, self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of batches\"\"\"\n",
    "        return len(self.dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = get_default_device()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DeviceDataLoader(train_dl, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = nn.Sequential(\n",
    "    # in: 3 x 64 x 64\n",
    "\n",
    "    nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.LeakyReLU(0.2, inplace=True),\n",
    "    # out: 64 x 32 x 32\n",
    "\n",
    "    nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "    nn.BatchNorm2d(128),\n",
    "    nn.LeakyReLU(0.2, inplace=True),\n",
    "    # out: 128 x 16 x 16\n",
    "\n",
    "    nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "    nn.BatchNorm2d(256),\n",
    "    nn.LeakyReLU(0.2, inplace=True),\n",
    "    # out: 256 x 8 x 8\n",
    "\n",
    "    nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "    nn.BatchNorm2d(512),\n",
    "    nn.LeakyReLU(0.2, inplace=True),\n",
    "    # out: 512 x 4 x 4\n",
    "\n",
    "    nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=0, bias=False),\n",
    "    # out: 1 x 1 x 1\n",
    "\n",
    "    nn.Flatten(),\n",
    "    nn.Sigmoid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = to_device(discriminator, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = nn.Sequential(\n",
    "    # in: latent_size x 1 x 1\n",
    "\n",
    "    nn.ConvTranspose2d(latent_size, 512, kernel_size=4, stride=1, padding=0, bias=False),\n",
    "    nn.BatchNorm2d(512),\n",
    "    nn.ReLU(True),\n",
    "    # out: 512 x 4 x 4\n",
    "\n",
    "    nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "    nn.BatchNorm2d(256),\n",
    "    nn.ReLU(True),\n",
    "    # out: 256 x 8 x 8\n",
    "\n",
    "    nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "    nn.BatchNorm2d(128),\n",
    "    nn.ReLU(True),\n",
    "    # out: 128 x 16 x 16\n",
    "\n",
    "    nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.ReLU(True),\n",
    "    # out: 64 x 32 x 32\n",
    "\n",
    "    nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "    nn.Tanh()\n",
    "    # out: 3 x 64 x 64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb = torch.randn(batch_size, latent_size, 1, 1) # random latent tensors\n",
    "fake_images = generator(xb)\n",
    "print(fake_images.shape)\n",
    "show_images(fake_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = to_device(generator, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_discriminator(real_images, opt_d):\n",
    "    # Clear discriminator gradients\n",
    "    opt_d.zero_grad()\n",
    "\n",
    "    # Pass real images through discriminator\n",
    "    real_preds = discriminator(real_images)\n",
    "    real_targets = torch.ones(real_images.size(0), 1, device=device)\n",
    "    real_loss = F.binary_cross_entropy(real_preds, real_targets)\n",
    "    real_score = torch.mean(real_preds).item()\n",
    "    \n",
    "    # Generate fake images\n",
    "    latent = torch.randn(batch_size, latent_size, 1, 1, device=device)\n",
    "    fake_images = generator(latent)\n",
    "\n",
    "    # Pass fake images through discriminator\n",
    "    fake_targets = torch.zeros(fake_images.size(0), 1, device=device)\n",
    "    fake_preds = discriminator(fake_images)\n",
    "    fake_loss = F.binary_cross_entropy(fake_preds, fake_targets)\n",
    "    fake_score = torch.mean(fake_preds).item()\n",
    "\n",
    "    # Update discriminator weights\n",
    "    loss = real_loss + fake_loss\n",
    "    loss.backward()\n",
    "    opt_d.step()\n",
    "    return loss.item(), real_score, fake_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator(opt_g):\n",
    "    # Clear generator gradients\n",
    "    opt_g.zero_grad()\n",
    "    \n",
    "    # Generate fake images\n",
    "    latent = torch.randn(batch_size, latent_size, 1, 1, device=device)\n",
    "    fake_images = generator(latent)\n",
    "    \n",
    "    # Try to fool the discriminator\n",
    "    preds = discriminator(fake_images)\n",
    "    targets = torch.ones(batch_size, 1, device=device)\n",
    "    loss = F.binary_cross_entropy(preds, targets)\n",
    "    \n",
    "    # Update generator weights\n",
    "    loss.backward()\n",
    "    opt_g.step()\n",
    "    \n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dir = 'painter_generation_1'\n",
    "os.makedirs(sample_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_samples(index, latent_tensors, show=True):\n",
    "    fake_images = generator(latent_tensors)\n",
    "    fake_fname = 'simp-img-{0:0=4d}.png'.format(index)\n",
    "    save_image(denorm(fake_images), os.path.join(sample_dir, fake_fname), nrow=8)\n",
    "    print('Saving', fake_fname)\n",
    "    if show:\n",
    "        fig, ax = plt.subplots(figsize=(8, 8))\n",
    "        ax.set_xticks([]); ax.set_yticks([])\n",
    "        ax.imshow(make_grid(fake_images.cpu().detach(), nrow=8).permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_latent = torch.randn(64, latent_size, 1, 1, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_samples(0, fixed_latent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, lr, start_idx=1):\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Losses & scores\n",
    "    losses_g = []\n",
    "    losses_d = []\n",
    "    real_scores = []\n",
    "    fake_scores = []\n",
    "    \n",
    "    # Create optimizers\n",
    "    opt_d = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "    opt_g = torch.optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for real_images, _ in (train_dl):\n",
    "            # Train discriminator\n",
    "            loss_d, real_score, fake_score = train_discriminator(real_images, opt_d)\n",
    "            # Train generator\n",
    "            loss_g = train_generator(opt_g)\n",
    "            \n",
    "        # Record losses & scores\n",
    "        losses_g.append(loss_g)\n",
    "        losses_d.append(loss_d)\n",
    "        real_scores.append(real_score)\n",
    "        fake_scores.append(fake_score)\n",
    "        \n",
    "        # Log losses & scores (last batch)\n",
    "        print(\"Epoch [{}/{}], loss_g: {:.4f}, loss_d: {:.4f}, real_score: {:.4f}, fake_score: {:.4f}\".format(\n",
    "            epoch+1, epochs, loss_g, loss_d, real_score, fake_score))\n",
    "    \n",
    "        # Save generated images\n",
    "        save_samples(epoch+start_idx, fixed_latent, show=False)\n",
    "    \n",
    "    return losses_g, losses_d, real_scores, fake_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.0002\n",
    "epochs = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = fit(epochs, lr,start_idx=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(generator.state_dict(), 'G_paint.pth')\n",
    "# torch.save(discriminator.state_dict(), 'D_paint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simp-img-0000.png\n",
      "simp-img-0001.png\n",
      "simp-img-0002.png\n",
      "simp-img-0003.png\n",
      "simp-img-0004.png\n",
      "simp-img-0005.png\n",
      "simp-img-0006.png\n",
      "simp-img-0007.png\n",
      "simp-img-0008.png\n",
      "simp-img-0009.png\n",
      "simp-img-0010.png\n",
      "simp-img-0011.png\n",
      "simp-img-0012.png\n",
      "simp-img-0013.png\n",
      "simp-img-0014.png\n",
      "simp-img-0015.png\n",
      "simp-img-0016.png\n",
      "simp-img-0017.png\n",
      "simp-img-0018.png\n",
      "simp-img-0019.png\n",
      "simp-img-0020.png\n",
      "simp-img-0021.png\n",
      "simp-img-0022.png\n",
      "simp-img-0023.png\n",
      "simp-img-0024.png\n",
      "simp-img-0025.png\n",
      "simp-img-0026.png\n",
      "simp-img-0027.png\n",
      "simp-img-0028.png\n",
      "simp-img-0029.png\n",
      "simp-img-0030.png\n",
      "simp-img-0031.png\n",
      "simp-img-0032.png\n",
      "simp-img-0033.png\n",
      "simp-img-0034.png\n",
      "simp-img-0035.png\n",
      "simp-img-0036.png\n",
      "simp-img-0037.png\n",
      "simp-img-0038.png\n",
      "simp-img-0039.png\n",
      "simp-img-0040.png\n",
      "simp-img-0041.png\n",
      "simp-img-0042.png\n",
      "simp-img-0043.png\n",
      "simp-img-0044.png\n",
      "simp-img-0045.png\n",
      "simp-img-0046.png\n",
      "simp-img-0047.png\n",
      "simp-img-0048.png\n",
      "simp-img-0049.png\n",
      "simp-img-0050.png\n",
      "simp-img-0051.png\n",
      "simp-img-0052.png\n",
      "simp-img-0053.png\n",
      "simp-img-0054.png\n",
      "simp-img-0055.png\n",
      "simp-img-0056.png\n",
      "simp-img-0057.png\n",
      "simp-img-0058.png\n",
      "simp-img-0059.png\n",
      "simp-img-0060.png\n",
      "simp-img-0061.png\n",
      "simp-img-0062.png\n",
      "simp-img-0063.png\n",
      "simp-img-0064.png\n",
      "simp-img-0065.png\n",
      "simp-img-0066.png\n",
      "simp-img-0067.png\n",
      "simp-img-0068.png\n",
      "simp-img-0069.png\n",
      "simp-img-0070.png\n",
      "simp-img-0071.png\n",
      "simp-img-0072.png\n",
      "simp-img-0073.png\n",
      "simp-img-0074.png\n",
      "simp-img-0075.png\n",
      "simp-img-0076.png\n",
      "simp-img-0077.png\n",
      "simp-img-0078.png\n",
      "simp-img-0079.png\n",
      "simp-img-0080.png\n",
      "simp-img-0081.png\n",
      "simp-img-0082.png\n",
      "simp-img-0083.png\n",
      "simp-img-0084.png\n",
      "simp-img-0085.png\n",
      "simp-img-0086.png\n",
      "simp-img-0087.png\n",
      "simp-img-0088.png\n",
      "simp-img-0089.png\n",
      "simp-img-0090.png\n",
      "simp-img-0091.png\n",
      "simp-img-0092.png\n",
      "simp-img-0093.png\n",
      "simp-img-0094.png\n",
      "simp-img-0095.png\n",
      "simp-img-0096.png\n",
      "simp-img-0097.png\n",
      "simp-img-0098.png\n",
      "simp-img-0099.png\n",
      "simp-img-0100.png\n",
      "simp-img-0101.png\n",
      "simp-img-0102.png\n",
      "simp-img-0103.png\n",
      "simp-img-0104.png\n",
      "simp-img-0105.png\n",
      "simp-img-0106.png\n",
      "simp-img-0107.png\n",
      "simp-img-0108.png\n",
      "simp-img-0109.png\n",
      "simp-img-0110.png\n",
      "simp-img-0111.png\n",
      "simp-img-0112.png\n",
      "simp-img-0113.png\n",
      "simp-img-0114.png\n",
      "simp-img-0115.png\n",
      "simp-img-0116.png\n",
      "simp-img-0117.png\n",
      "simp-img-0118.png\n",
      "simp-img-0119.png\n",
      "simp-img-0120.png\n",
      "simp-img-0121.png\n",
      "simp-img-0122.png\n",
      "simp-img-0123.png\n",
      "simp-img-0124.png\n",
      "simp-img-0125.png\n",
      "simp-img-0126.png\n",
      "simp-img-0127.png\n",
      "simp-img-0128.png\n",
      "simp-img-0129.png\n",
      "simp-img-0130.png\n",
      "simp-img-0131.png\n",
      "simp-img-0132.png\n",
      "simp-img-0133.png\n",
      "simp-img-0134.png\n",
      "simp-img-0135.png\n",
      "simp-img-0136.png\n",
      "simp-img-0137.png\n",
      "simp-img-0138.png\n",
      "simp-img-0139.png\n",
      "simp-img-0140.png\n",
      "simp-img-0141.png\n",
      "simp-img-0142.png\n",
      "simp-img-0143.png\n",
      "simp-img-0144.png\n",
      "simp-img-0145.png\n",
      "simp-img-0146.png\n",
      "simp-img-0147.png\n",
      "simp-img-0148.png\n",
      "simp-img-0149.png\n",
      "simp-img-0150.png\n",
      "simp-img-0151.png\n",
      "simp-img-0152.png\n",
      "simp-img-0153.png\n",
      "simp-img-0154.png\n",
      "simp-img-0155.png\n",
      "simp-img-0156.png\n",
      "simp-img-0157.png\n",
      "simp-img-0158.png\n",
      "simp-img-0159.png\n",
      "simp-img-0160.png\n",
      "simp-img-0161.png\n",
      "simp-img-0162.png\n",
      "simp-img-0163.png\n",
      "simp-img-0164.png\n",
      "simp-img-0165.png\n",
      "simp-img-0166.png\n",
      "simp-img-0167.png\n",
      "simp-img-0168.png\n",
      "simp-img-0169.png\n",
      "simp-img-0170.png\n",
      "simp-img-0171.png\n",
      "simp-img-0172.png\n",
      "simp-img-0173.png\n",
      "simp-img-0174.png\n",
      "simp-img-0175.png\n",
      "simp-img-0176.png\n",
      "simp-img-0177.png\n",
      "simp-img-0178.png\n",
      "simp-img-0179.png\n",
      "simp-img-0180.png\n",
      "simp-img-0181.png\n",
      "simp-img-0182.png\n",
      "simp-img-0183.png\n",
      "simp-img-0184.png\n",
      "simp-img-0185.png\n",
      "simp-img-0186.png\n",
      "simp-img-0187.png\n",
      "simp-img-0188.png\n",
      "simp-img-0189.png\n",
      "simp-img-0190.png\n",
      "simp-img-0191.png\n",
      "simp-img-0192.png\n",
      "simp-img-0193.png\n",
      "simp-img-0194.png\n",
      "simp-img-0195.png\n",
      "simp-img-0196.png\n",
      "simp-img-0197.png\n",
      "simp-img-0198.png\n",
      "simp-img-0199.png\n",
      "simp-img-0200.png\n",
      "simp-img-0201.png\n",
      "simp-img-0202.png\n",
      "simp-img-0203.png\n",
      "simp-img-0204.png\n",
      "simp-img-0205.png\n",
      "simp-img-0206.png\n",
      "simp-img-0207.png\n",
      "simp-img-0208.png\n",
      "simp-img-0209.png\n",
      "simp-img-0210.png\n",
      "simp-img-0211.png\n",
      "simp-img-0212.png\n",
      "simp-img-0213.png\n",
      "simp-img-0214.png\n",
      "simp-img-0215.png\n",
      "simp-img-0216.png\n",
      "simp-img-0217.png\n",
      "simp-img-0218.png\n",
      "simp-img-0219.png\n",
      "simp-img-0220.png\n",
      "simp-img-0221.png\n",
      "simp-img-0222.png\n",
      "simp-img-0223.png\n",
      "simp-img-0224.png\n",
      "simp-img-0225.png\n",
      "simp-img-0226.png\n",
      "simp-img-0227.png\n",
      "simp-img-0228.png\n",
      "simp-img-0229.png\n",
      "simp-img-0230.png\n",
      "simp-img-0231.png\n",
      "simp-img-0232.png\n",
      "simp-img-0233.png\n",
      "simp-img-0234.png\n",
      "simp-img-0235.png\n",
      "simp-img-0236.png\n",
      "simp-img-0237.png\n",
      "simp-img-0238.png\n",
      "simp-img-0239.png\n",
      "simp-img-0240.png\n",
      "simp-img-0241.png\n",
      "simp-img-0242.png\n",
      "simp-img-0243.png\n",
      "simp-img-0244.png\n",
      "simp-img-0245.png\n",
      "simp-img-0246.png\n",
      "simp-img-0247.png\n",
      "simp-img-0248.png\n",
      "simp-img-0249.png\n",
      "simp-img-0250.png\n",
      "simp-img-0251.png\n",
      "simp-img-0252.png\n",
      "simp-img-0253.png\n",
      "simp-img-0254.png\n",
      "simp-img-0255.png\n",
      "simp-img-0256.png\n",
      "simp-img-0257.png\n",
      "simp-img-0258.png\n",
      "simp-img-0259.png\n",
      "simp-img-0260.png\n",
      "simp-img-0261.png\n",
      "simp-img-0262.png\n",
      "simp-img-0263.png\n",
      "simp-img-0264.png\n",
      "simp-img-0265.png\n",
      "simp-img-0266.png\n",
      "simp-img-0267.png\n",
      "simp-img-0268.png\n",
      "simp-img-0269.png\n",
      "simp-img-0270.png\n",
      "simp-img-0271.png\n",
      "simp-img-0272.png\n",
      "simp-img-0273.png\n",
      "simp-img-0274.png\n",
      "simp-img-0275.png\n",
      "simp-img-0276.png\n",
      "simp-img-0277.png\n",
      "simp-img-0278.png\n",
      "simp-img-0279.png\n",
      "simp-img-0280.png\n",
      "simp-img-0281.png\n",
      "simp-img-0282.png\n",
      "simp-img-0283.png\n",
      "simp-img-0284.png\n",
      "simp-img-0285.png\n",
      "simp-img-0286.png\n",
      "simp-img-0287.png\n",
      "simp-img-0288.png\n",
      "simp-img-0289.png\n",
      "simp-img-0290.png\n",
      "simp-img-0291.png\n",
      "simp-img-0292.png\n",
      "simp-img-0293.png\n",
      "simp-img-0294.png\n",
      "simp-img-0295.png\n",
      "simp-img-0296.png\n",
      "simp-img-0297.png\n",
      "simp-img-0298.png\n",
      "simp-img-0299.png\n",
      "simp-img-0300.png\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "\n",
    "frame = cv2.imread(\"painter_generation_1/simp-img-0000.png\")\n",
    "height, width, layers = frame.shape\n",
    "\n",
    "\n",
    "#insering frames into video\n",
    "video = cv2.VideoWriter(\"./painter_gan1.avi\", 0,20, (width, height)) \n",
    "files = os.listdir(\"painter_generation_1\")\n",
    "files.sort()\n",
    "for i in files: \n",
    "   video.write(cv2.imread(\"painter_generation_1/\"+i)) \n",
    "   print(i)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
